chi <- matrix(nrow = d, ncol = d)
for (i in 1:(d - 1)) {
for (j in (i + 1):d) {
chi[i,j] <- chi_est(X[,i], X[,j], q)
}
}
weight_matrix <- matrix(nrow=0,ncol=3)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
weight_matrix <- rbind(weight_matrix,c(i,j,-log(chi[i,j])))
}
}
weighted_graph <- graph.data.frame(weight_matrix[,1:2],directed=F)
E(weighted_graph)$weight <- weight_matrix[,3]
mst <- minimum.spanning.tree(weighted_graph)
mst
}
learn_tree(X, 0.9)
plot(learn_tree(X, 0.9))
dim(replicate(7,sim_X(100)))
## Function for estimating probability of recovering the tree.
## K is the number of simulations to base each probability estimate on,
## qvec is the values of q to use for the structure learning, and n is
## the number of times the process is sampled (i.e. sampling frequency is 1/n)
experiment_threshold <- function(K, qvec, n) {
X <- replicate(K,sim_X(n))
d <- dim(X)[2]
results <- vector(length=length(qvec))
for (i in 1:length(qvec)) {
matches <- sapply(1:K,function(k) length(E(graph.intersection(Tree,learn_tree(X[,,k],qvec[i]))))==d-1)
results[i] <- mean(matches)
}
results
}
experiment_threshold(10,.9,1000)
K <- 10
q_range <- seq(.8,.99,.01)
results5000 <- experiment_threshold(K,q_range,5000)
results2000 <- experiment_threshold(K,q_range,2000)
results1000 <- experiment_threshold(K,q_range,1000)
results500 <- experiment_threshold(K,q_range,500)
results200 <- experiment_threshold(K,q_range,200)
# Plotting
plot(q_range,results200,type="l",ylim=c(0,1), col = "red")
lines(q_range,results500, col = "orange")
lines(q_range,results1000, col = "darkgreen")
lines(q_range,results2000, col = "blue")
lines(q_range,results5000, col = "purple")
## Function for estimating probability of recovering the tree.
## K is the number of simulations to base each probability estimate on,
## qvec is the values of q to use for the structure learning, and n is
## the number of times the process is sampled (i.e. sampling frequency is 1/n)
est_probs <- function(K, qvec, n) {
X <- replicate(K, sim_X(n))
d <- dim(X)[2]
results <- vector(length = length(qvec))
for (i in 1:length(qvec)) {
matches <- sapply(1:K, function(k) length(E(graph.intersection(Tree, learn_tree(X[,,k], qvec[i])))) == d - 1)
results[i] <- mean(matches)
}
results
}
K <- 10
q_range <- seq(.5, .9, .1)
n_range <- c(200, 500, 1000, 2000, 5000)
results <- matrix(nrow = length(q_range), ncol = length(n_range))
for (j in 1:length(n_range)) {
results[,j] <- est_probs(K, q_range, n_range[j])
}
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1))
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
toString(n_range)
legend("center", toString(n_range))
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
legend("center", toString(n_range), col = length(n_range))
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
legend("bottom", toString(n_range), col = length(n_range))
results5000 <- experiment_threshold(K,q_range,5000)
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
legend("bottom", toString(n_range), fill = length(n_range))
legend("bottom", n_range, fill = length(n_range))
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
toString(n_range)
legend("bottom", c("200", "500", "1000", "2000", "5000"), fill = length(n_range))
legend("bottom", c("200", "500", "1000", "2000", "5000"), col = length(n_range))
matplot(q_range, results, ylim = c(0,1), type = "l")
legend("bottom", c("200", "500", "1000", "2000", "5000"), col = length(n_range))
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
legend("right", c("200", "500", "1000", "2000", "5000"), col = length(n_range))
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
legend("top", c("200", "500", "1000", "2000", "5000"), col = length(n_range))
legend("top", c("200", "500", "1000", "2000", "5000"), col = length(n_range), cex=0.8)
legend("right", c("200", "500", "1000", "2000", "5000"), col = length(n_range), fill = length(n_range), cex=0.8)
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
legend("right", c("200", "500", "1000", "2000", "5000"), col = length(n_range), fill = length(n_range), cex=0.8)
legend("right", c("200", "500", "1000", "2000", "5000"), col = seq_len(length(n_range)), fill = seq_len(length(n_range)), cex=0.8)
## Plotting probabilities as a function of q
matplot(q_range, results, ylim = c(0,1), type = "l")
legend("right", c("200", "500", "1000", "2000", "5000"), col = seq_len(length(n_range)), fill = seq_len(length(n_range)), cex=0.8)
N <- length(n_range)
layout(matrix(c(1,2),nrow=1), width=c(4,1))
par(mar=c(5,4,4,0))
matplot(q_range, results, ylim = c(0,1), type = "l", xlab = "$q$", ylab = "Recovery probability")
par(mar=c(5,0,4,2))
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("center", c("200", "500", "1000", "2000", "5000"),
col = seq_len(N), fill = seq_len(N), cex=0.8)
legend("right", c("200", "500", "1000", "2000", "5000"),
col = seq_len(N), fill = seq_len(N), cex=0.8)
N <- length(n_range)
layout(matrix(c(1,2),nrow=1), width=c(4,1))
par(mar=c(5,4,4,0))
matplot(q_range, results, ylim = c(0,1), type = "l", xlab = "$q$", ylab = "Recovery probability")
par(mar=c(5,0,4,2))
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("right", c("200", "500", "1000", "2000", "5000"),
col = seq_len(N), fill = seq_len(N), cex=0.8)
matplot(q_range, results, ylim = c(0,1), type = "l", xlab = "q", ylab = "Recovery probability")
N <- length(n_range)
layout(matrix(c(1,2),nrow=1), width=c(4,1))
par(mar=c(5,4,4,0))
matplot(q_range, results, ylim = c(0,1), type = "l", xlab = "q", ylab = "Recovery probability")
par(mar=c(5,0,4,2))
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("right", c("200", "500", "1000", "2000", "5000"),
col = seq_len(N), fill = seq_len(N), cex=0.8)
getwd()
setwd("~/Documents/Matematik/Repositories/levy_graphical_models")
getwd()
## Load data
log_prices <- read.csv("data_all.csv")
str(log_prices)
log_prices["AAPL"]
log_prices[c("AAPl", "UPS")]
log_prices[c("AAPL", "UPS")]
str(log_prices(c("AAPL", "UPS")))
str(log_prices[c("AAPL", "UPS")])
selected_tickers <- c("AAPL", "MSFT",
"OXY",  "CVX",
"UPS",  "FDX",
"KO",   "PEP",
"HD",   "TGT",
"JPM",  "WFC")
selected_data <- log_prices[selected_tickers]
str(selected_data)
str(diff(selected_data))
str(matrix(selected_data))
str(selected_tickers.matrix())
str(data.matrix(selected_data))
## Calculating increments
data_mat <- data.matrix(selected_data)
incs <- diff(data_mat)
str(incs)
plot(incs[,1])
hist(incs[,1])
incs[1:5,1]
require(igraph)
increments <- function(X) {
n <- length(X)
X-c(0,X[1:(n-1)])
}
## Applies empirical CDF of increments of X to the increments
ecdf_transform <- function(X) {
n <- length(X)
rank(increments(X)) / (n + 1)
}
## Function for estimating chi based on positive quadrant
positive_quadrant <- function(X1, X2, q) {
n <- length(X1)
Z1 <- ecdf_transform(X1)
Z2 <- ecdf_transform(X2)
sum((Z1 > q) * (Z2 > q)) / (n * (1 - q))
}
## Function for estimation chi based on all quadrants
chi_est <- function(X1, X2, q) {
res <- 0
for (s1 in c(-1,1)) {
for (s2 in c(-1,1)) {
res <- res + positive_quadrant(s1 * X1, s2 * X2, q)
}
}
res
}
## Function for learning the tree
learn_tree <- function(X, q) {
d = ncol(X)
chi <- matrix(nrow = d, ncol = d)
for (i in 1:(d - 1)) {
for (j in (i + 1):d) {
chi[i,j] <- chi_est(X[,i], X[,j], q)
}
}
weight_matrix <- matrix(nrow=0,ncol=3)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
weight_matrix <- rbind(weight_matrix,c(i,j,-log(chi[i,j])))
}
}
weighted_graph <- graph.data.frame(weight_matrix[,1:2],directed=F)
E(weighted_graph)$weight <- weight_matrix[,3]
mst <- minimum.spanning.tree(weighted_graph)
mst
}
## Choosing q
q = 0.94
## Estimate graph.
G = learn_tree(data.matrix(selected_data), q)
G
plot(G,asp=0)
plot(G, asp=0, vertex.label = selected_tickers)
## Estimate and plot the tree
tree = learn_tree(data.matrix(selected_data), q)
plot(tree, asp=0, vertex.label = selected_tickers)
nrow(selected_data)
## Calculating increments
incs <- diff(data.matrix(selected_data))
str(incs)
incs[1:5,1]
nrow(incs)
## Some setup
n <- nrow(incs)
d <- ncol(incs)
n // 2
n / 2
round(n/2)
sub_size <- round(n / 2) #Size of random subsamples
N <- 10000 #Number of subsampling iterations
edge_counts <- matrix(0, nrow = d, ncol = d)
edge_counts
replicate(sample(1:n, size = sub_size, replace = TRUE), 10)
replicate(10, sample(1:n, size = sub_size, replace = TRUE))
q_range <- c(0.925, 0.95) #q is sampled uniformly in this range
##Doing the subsampling
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
str(index_mat)
str(q_vec)
tree
as_edgelist(tree)
as.list(as_edgelist(tree))
as_edgelist(tree) == c("1", "5")
as_edgelist(tree)
c("1", "5") %in% as_edgelist(tree)
c("1", "6") %in% as_edgelist(tree)
c("1", "5") == as_edgelist(tree)[1,]
all(c("1", "5") == as_edgelist(tree)[1,])
all(c("1", "6") == as_edgelist(tree)[1,])
all(c("5", "1") == as_edgelist(tree)[1,])
all(c("4", "5") == as_edgelist(tree)[1,])
all(c("4", "6") == as_edgelist(tree)[1,])
toString(as_edgelist(tree)[1,])
apply(as_edgelist(tree),1,tostring)
apply(as_edgelist(tree),1,toString)
"1, 5" %in% apply(as_edgelist(tree),1,toString)
"5, 1" %in% apply(as_edgelist(tree),1,toString)
paste(1,2)
paste(1,2, sep = ", ")
str(index_mat)
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
update_counts <- function(incs, q) {
X <- apply(incs, 2, cumsum)
tree <- learn_tree(X, q)
edges <- apply(as_edgelist(tree),1,toString)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
edge_counts[i,j] <<- edge_counts[i,j] + paste(i, j, sep = ", ") %in% edges
}
}
}
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
## Calculating increments
selected_incs <- diff(data.matrix(selected_data))
edge_counts <- matrix(0, nrow = d, ncol = d)
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
log_prices <- read.csv("data_all.csv")
#############################
### Selecting some stocks ###
#############################
## Selecting certain stocks
selected_tickers <- c("AAPL", "MSFT",
"OXY",  "CVX",
"UPS",  "FDX",
"KO",   "PEP",
"HD",   "TGT",
"JPM",  "WFC")
selected_data <- log_prices[selected_tickers]
#####################
### Tree learning ###
#####################
## Choosing q
q <- 0.94
## Estimating and plotting the tree
tree <- learn_tree(data.matrix(selected_data), q)
plot(tree, asp = 0, vertex.label = selected_tickers)
###################
### Subsampling ###
###################
## Calculating increments
selected_incs <- diff(data.matrix(selected_data))
## Some setup
n <- nrow(selected_incs)
d <- ncol(selected_incs)
sub_size <- round(n / 2) #Size of random subsamples
N <- 10000 #Number of subsampling iterations
edge_counts <- matrix(0, nrow = d, ncol = d)
q_range <- c(0.925, 0.95) #q is sampled uniformly in this range
##Doing the subsampling
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
update_counts <- function(incs, q) {
X <- apply(incs, 2, cumsum)
tree <- learn_tree(X, q)
edges <- apply(as_edgelist(tree),1,toString)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
edge_counts[i,j] <<- edge_counts[i,j] + paste(i, j, sep = ", ") %in% edges
}
}
}
N <- 100 #Number of subsampling iterations
N <- 10 #Number of subsampling iterations
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
edge_counts
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
edge_counts
##Doing the subsampling
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
edge_counts <- matrix(0, nrow = d, ncol = d)
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
edge_counts
require(igraph)
increments <- function(X) {
n <- length(X)
X-c(0,X[1:(n-1)])
}
## Applies empirical CDF of increments to the increments
ecdf_transform <- function(incs) {
n <- length(incs)
rank(incs) / (n + 1)
}
## Function for estimating chi based on positive quadrant
positive_quadrant <- function(incs1, incs2, q) {
n <- length(incs1)
Z1 <- ecdf_transform(incs1)
Z2 <- ecdf_transform(incs2)
sum((Z1 > q) * (Z2 > q)) / (n * (1 - q))
}
## Function for estimation chi based on all quadrants
chi_est <- function(incs1, incs2, q) {
res <- 0
for (s1 in c(-1,1)) {
for (s2 in c(-1,1)) {
res <- res + positive_quadrant(s1 * incs1, s2 * incs2, q)
}
}
res
}
## Function for learning the tree
learn_tree <- function(incs, q) {
d = ncol(incs)
chi <- matrix(nrow = d, ncol = d)
for (i in 1:(d - 1)) {
for (j in (i + 1):d) {
chi[i,j] <- chi_est(incs[,i], incs[,j], q)
}
}
weight_matrix <- matrix(nrow=0,ncol=3)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
weight_matrix <- rbind(weight_matrix,c(i,j,-log(chi[i,j])))
}
}
weighted_graph <- graph.data.frame(weight_matrix[,1:2],directed=F)
E(weighted_graph)$weight <- weight_matrix[,3]
mst <- minimum.spanning.tree(weighted_graph)
mst
}
## Loading data - make sure to set working directory or modify the path
log_prices <- read.csv("data_all.csv")
selected_tickers <- c("AAPL", "MSFT",
"OXY",  "CVX",
"UPS",  "FDX",
"KO",   "PEP",
"HD",   "TGT",
"JPM",  "WFC")
selected_data <- log_prices[selected_tickers]
## Calculating increments
selected_incs <- diff(data.matrix(selected_data))
## Choosing q
q <- 0.94
## Estimating and plotting the tree
tree <- learn_tree(data.matrix(selected_data), q)
plot(tree, asp = 0, vertex.label = selected_tickers)
## Estimating and plotting the tree
tree <- learn_tree(selected_incs, q)
plot(tree, asp = 0, vertex.label = selected_tickers)
## Some setup
n <- nrow(selected_incs)
d <- ncol(selected_incs)
sub_size <- round(n / 2) #Size of random subsamples
N <- 10 #Number of subsampling iterations
edge_counts <- matrix(0, nrow = d, ncol = d)
q_range <- c(0.925, 0.95) #q is sampled uniformly in this range
##Doing the subsampling
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
update_counts <- function(incs, q) {
tree <- learn_tree(incs, q)
edges <- apply(as_edgelist(tree),1,toString)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
edge_counts[i,j] <<- edge_counts[i,j] + paste(i, j, sep = ", ") %in% edges
}
}
}
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
edge_counts
N <- 100 #Number of subsampling iterations
edge_counts <- matrix(0, nrow = d, ncol = d)
q_range <- c(0.925, 0.95) #q is sampled uniformly in this range
##Doing the subsampling
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
update_counts <- function(incs, q) {
tree <- learn_tree(incs, q)
edges <- apply(as_edgelist(tree),1,toString)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
edge_counts[i,j] <<- edge_counts[i,j] + paste(i, j, sep = ", ") %in% edges
}
}
}
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
edge_counts
require(tictoc)
tic()
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
update_counts <- function(incs, q) {
tree <- learn_tree(incs, q)
edges <- apply(as_edgelist(tree),1,toString)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
edge_counts[i,j] <<- edge_counts[i,j] + paste(i, j, sep = ", ") %in% edges
}
}
}
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
toc()
edge_counts <- matrix(0, nrow = d, ncol = d)
tic()
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
toc()
N <- 10000 #Number of subsampling iterations
edge_counts
tree.ecount()
ecount(tree)
N
edge_counts <- matrix(0, nrow = d, ncol = d)
n <- nrow(selected_incs)
d <- ncol(selected_incs)
sub_size <- round(n / 2) #Size of random subsamples
N <- 10000 #Number of subsampling iterations
edge_counts <- matrix(0, nrow = d, ncol = d)
q_range <- c(0.925, 0.95) #q is sampled uniformly in this range
## Function for updating edge_counts
update_counts <- function(incs, q) {
tree <- learn_tree(incs, q)
edges <- apply(as_edgelist(tree),1,toString)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
edge_counts[i,j] <<- edge_counts[i,j] + paste(i, j, sep = ", ") %in% edges
}
}
}
##Doing the subsampling
tic()
index_mat <- replicate(N, sample(1:n, size = sub_size, replace = FALSE))
q_vec <- runif(N, min = q_range[1], max = q_range[2])
sapply(1:N, function(j) update_counts(selected_incs[index_mat[,j],], q_vec[j]))
toc()
edge_counts
## Plotting fully connected graph with edge widths proportional to edge_counts
weights_fully <- matrix(nrow=0,ncol=3)
for (i in 1:(d-1)) {
for (j in (i+1):d) {
weights_fully <- rbind(weights_fully, c(i, j, edge_counts[i,j]))
}
}
graph_fully <- graph.data.frame(weights_fully[,1:2],directed=F)
E(graph_fully)$weight <- weights_fully[,3]
plot(graph_fully)
mst <- minimum.spanning.tree(weighted_graph, edge.width = weight_matrix[,3])
plot(graph_fully,edge.width = weight_matrix[,3])
plot(graph_fully,edge.width = weights_fully[,3])
plot(graph_fully,edge.width = weights_fully[,3] / N)
plot(graph_fully,edge.width = weights_fully[,3] / (N / 5))
plot(graph_fully,edge.width = weights_fully[,3] / (N / 5), vertex.label = selected_tickers)
plot(graph_fully,edge.width = weights_fully[,3] / (N / 5), vertex.label = selected_tickers)
